{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documented-stage",
   "metadata": {},
   "source": [
    "# OpenStreetMap Udacity Project\n",
    "\n",
    "## Map area\n",
    "\n",
    "### Reno, Nevada\n",
    "\n",
    "https://www.openstreetmap.org/relation/170120\n",
    "\n",
    "I chose this area because I recently moved to Nevada and I thought it would be interesting to do some research on a city close to me. There is so much rural area around me that this seemed like a safe bet for finding interesting information worthy of wrangling! It is worth noting that this project tested me. I felt I had a reasonable, novice-level, grasp of programming in C++. I question everything now. I struggled through this and had to refer to stackoverflow.com quite often for what feels like basic things. I believe I have accomplished what the goal of the project is, to show a general understanding of wrangling and perform general queries on said wrangled data. I hope this project also conveys this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-windows",
   "metadata": {},
   "source": [
    "## This is the sample code provided by Udacity which was used for sample.osm creation.\n",
    "\n",
    "Some minor changes were done to account for a bytes/str error I was recieving.\n",
    "\n",
    "### sample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infrared-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"reno.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write(b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n') # ADDED \"b\" TO ACCOUNT FOR BYTES/STRING ERROR *****************\n",
    "    output.write(b'<osm>\\n  ') # ADDED \"b\" TO ACCOUNT FOR BYTES/STRING ERROR ***************************************************\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write(b'</osm>') # ADDED \"b\" TO ACCOUNT FOR BYTES/STRING ERROR *********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-shanghai",
   "metadata": {},
   "source": [
    "## This is code provided by Udacity with the necessary updates for the audit procedure.\n",
    "\n",
    "While analyzing the output of the sample.osm, some problems encountered were found in addresses, zip codes, and city names. Addresses were not consistent with the use of several abbreviations, zip codes may have white space characters and extra info beyond five numbers, and Reno was inconsistently listed as a city. The audit.py file provided by Udacity was updated to audit these issues. Though there are many more potential issues, for this project we will focus on general data cleaning for query purposes. We will focus on cleaning street names, zip codes, and adding Reno to all city fields as data was only for the Reno area. I left a long trailing \"*\" (asterisk) to indicate changes I made in the files provided.\n",
    "\n",
    "### audit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "patient-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"reno.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "#UPDATED EXPECTED AS WELL *************************************************************************************************************\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Loop\", \"Way\", \"View\", \"Circle\", \"Row\", \"CreekTrail\", \"40\",\n",
    "            \"Terrace\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE, UPDATED *****************************************************************************************************\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"st\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"blvd:\": \"Boulevard\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"pkwy\": \"Parkway\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"pl\": \"Place\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Pl.\": \"Place\",\n",
    "            \"Dr\": \"Drive\"\n",
    "            }\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "    return street_types #FIXED\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(OSMFILE):\n",
    "    osm_file = open(OSMFILE, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    postal_code_types = defaultdict(set)\n",
    "    \n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "                if is_postal_code(tag):\n",
    "                    audit_postal_code(postal_code_types, tag.attrib['v'])\n",
    "\n",
    "    osm_file.close()\n",
    "    return street_types, postal_code_types\n",
    "\n",
    "\n",
    "# YOUR CODE STARTS **********************************************************************************************************************************************************************************************************\n",
    "\n",
    "\n",
    "# CHARS FOR POSTAL CODE\n",
    "def is_postal_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "# DICTIONARY FOR POSTAL CODES MODELED FROM audit_steet_type\n",
    "def audit_postal_code(postal_code_types, postal_code):  \n",
    "    if not postal_code.isupper() or ' ' not in postal_code:\n",
    "        postal_code_types['case_whitespace_problems'].add(postal_code)\n",
    "    else:\n",
    "        postal_code_types['other'].add(postal_code)\n",
    "    return postal_code_types\n",
    "\n",
    "\n",
    "# POSTAL CODE TEST MODELED FROM test\n",
    "def postal_code_test(): \n",
    "    postcode_types = audit(OSMFILE)[1]\n",
    "    pprint.pprint(dict(postcode_types))\n",
    "\n",
    "    for postcode_type, postcodes in postcode_types.items():\n",
    "        for postcode in postcodes:\n",
    "            better_postcode = update_postal_code(postcode)\n",
    "            print(postcode, \"=>\", better_postcode)\n",
    "            \n",
    "\n",
    "# YOUR CODE ENDS **********************************************************************************************************************************************************************************************************\n",
    "  \n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)[0]#ADDED FOR DICT, FIXED\n",
    "    len(st_types) == 5\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print(name, \"=>\", better_name)\n",
    "            if name == \"West Lexington St.\":\n",
    "                better_name == \"West Lexington Street\"\n",
    "            if name == \"Baldwin Rd.\":\n",
    "                better_name == \"Baldwin Road\"\n",
    "                \n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    test()\n",
    "#    postal_code_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-landing",
   "metadata": {},
   "source": [
    "Some of this is adopted from audit.py, such as \"expected\" and \"mapping\". These helper functions will be imported for use in data.py. I also found it helpful to isolate code that worked while troubleshooting my code and it just became easier to import this file for this purpose.\n",
    "\n",
    "## fix.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "proved-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"reno.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "#UPDATED EXPECTED AS WELL\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Loop\", \"Way\", \"View\", \"Circle\", \"Row\", \"CreekTrail\", \"40\",\n",
    "            \"Terrace\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE, UPDATED\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"st\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"blvd:\": \"Boulevard\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"pkwy\": \"Parkway\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"pl\": \"Place\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Pl.\": \"Place\",\n",
    "            \"Dr\": \"Drive\"\n",
    "            }\n",
    "\n",
    "# DICTIONARY FOR POSTAL CODES MODELED FROM audit_steet_type\n",
    "def audit_postal_code(postal_code_types, postal_code):  \n",
    "    if not postal_code.isupper() or ' ' not in postal_code:\n",
    "        postal_code_types['case_whitespace_problems'].add(postal_code)\n",
    "    else:\n",
    "        postal_code_types['other'].add(postal_code)\n",
    "    return postal_code_types\n",
    "\n",
    "# REMOVE WHITESPACE OR ANY EXTRA CHARS BEYOND 5\n",
    "def update_postal_code(postal_code):\n",
    "    postal_code = postal_code.upper()\n",
    "    if ' ' not in postal_code:\n",
    "        if len(postal_code) != 5:\n",
    "            postal_code = postal_code[0:5]\n",
    "    return postal_code # changed from - postal_code\n",
    "\n",
    "# UPDATE STREET NAMES\n",
    "mapping_keys = []\n",
    "for k,v in mapping.items():\n",
    "    mapping_keys.append(k)\n",
    "\n",
    "def update_names(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if name == 'sparks place':\n",
    "        return name.title()\n",
    "    elif name == 'Sparks Square - 3':\n",
    "        return 'Sparks Square'\n",
    "    elif m:\n",
    "        bad_suffix = m.group()\n",
    "        if m.group() in mapping_keys: #l\n",
    "            good_suffix = mapping[bad_suffix]\n",
    "            return re.sub(bad_suffix,good_suffix,name)\n",
    "        else:\n",
    "            return name\n",
    "    else: \n",
    "        return name\n",
    "\n",
    "# ADD RENO AS CITY IF NOT THERE OR MISSPELLED\n",
    "def add_reno():\n",
    "    osm_file = open(OSMFILE, \"r\")  \n",
    "    city_list = set()\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if tag.attrib['k'] == \"addr:city\" and tag.attrib['v'] != \"Reno\":\n",
    "                    city_list.add(tag.attrib['v'])\n",
    "    return city_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    add_reno()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-banana",
   "metadata": {},
   "source": [
    "## This is code provided by Udacity with the necessary updates for the .osm to .csv  procedure.\n",
    "\n",
    "Utilized the code from data.py, as given by Udacity, to parse .osm into .csv files. Have included fix.py as an import. fix.py contains functions for cleaning streets, zip codes, and city names. Encountered a problem with the UnicodeDictWriter function which required updating to be compatible with Python 3. Used the update provided by WGU. I left a long trailing \"*\" (asterisk) to indicate changes I made in the files provided.\n",
    "\n",
    "\n",
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "qualified-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "import fix #IMPORT fix.py FOR CLEANING FUNCTIONS AND WILL ADD RENO TO ANY MISSED CITY FIELD\n",
    "\n",
    "OSM_PATH = \"reno.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "#HELPER FUNCTIONS FROM fix.py FOR CLEANING STREET AND POSTAL CODE ***************************************************************************\n",
    "def update_tags(tag):\n",
    "    if tag['key'] == \"street\":\n",
    "        tag['value'] = fix.update_name(tag['value'], fix.mapping)\n",
    "\n",
    "    elif tag['key'] == \"postcode\":\n",
    "        tag['value'] = fix.update_postal_code(tag['value'])\n",
    "\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "# YOUR CODE STARTS HERE **********************************************************************************************************************************************************************************************************\n",
    "# PARSE AND CREATE .csv's CALL UPDATE_TAGS WITHIN FIX.PY FOR CLEANING FUNCTION, THIS WORKS!!!! 5:30\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for field in NODE_FIELDS: \n",
    "            v_field = element.attrib[field]\n",
    "            node_attribs[field] = v_field\n",
    "            \n",
    "        for child in element: \n",
    "            tag = {}\n",
    "            vk = child.attrib['k']\n",
    "            tag['id'] = node_attribs['id']\n",
    "            \n",
    "            if PROBLEMCHARS.search(vk): \n",
    "                continue\n",
    "                \n",
    "            elif LOWER_COLON.search(vk): \n",
    "                vk_split = vk.split(':', 1)\n",
    "                tag['type'] = vk_split[0]\n",
    "                tag['key'] = vk_split[1]\n",
    "                tag['value'] = child.attrib['v']\n",
    "            \n",
    "            else: \n",
    "                tag['key'] = vk\n",
    "                tag['value'] = child.attrib['v']\n",
    "                tag['type'] = default_tag_type\n",
    "\n",
    "            update_tags(tag) # FIX.PY \n",
    "            tags.append(tag) \n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    elif element.tag == 'way':\n",
    "\n",
    "        for field in WAY_FIELDS: \n",
    "            v_field = element.attrib[field]\n",
    "            way_attribs[field] = v_field\n",
    "        \n",
    "        counter = 0\n",
    "        for child in element:\n",
    "            tag = {}\n",
    "            if child.tag == 'tag':  \n",
    "                vk = child.attrib['k']\n",
    "                tag['id'] = way_attribs['id']\n",
    "                \n",
    "                if PROBLEMCHARS.search(vk): \n",
    "                    continue\n",
    "                    \n",
    "                elif LOWER_COLON.search(vk): \n",
    "                    vk_split = vk.split(':', 1)\n",
    "                    tag['type'] = vk_split[0]\n",
    "                    tag['key'] = vk_split[1]\n",
    "                    tag['value'] = child.attrib['v']\n",
    "                \n",
    "                else: \n",
    "                    tag['key'] = vk\n",
    "                    tag['value'] = child.attrib['v']\n",
    "                    tag['type'] = default_tag_type\n",
    "\n",
    "                update_tags(tag) # FIX.PY\n",
    "                tags.append(tag) \n",
    "            \n",
    "            if child.tag =='nd': \n",
    "                way_node = {}\n",
    "                way_node['id'] = way_attribs['id']\n",
    "                way_node['node_id'] = child.attrib['ref']\n",
    "                way_node['position'] = counter\n",
    "                counter += 1\n",
    "                way_nodes.append(way_node)\n",
    "                \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "# WORKS, FINALLY 5:30 ******************************************************************************************************************\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "# FIXED UNICODEDICTWRITER ERROR PER INFO FROM WGU CHATTER, PYTHON 2 ERRORS *******************************************************************************\n",
    "        \n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: v for k, v in row.items()\n",
    "        })\n",
    "    \n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "#ADDED UTF FOR OPEN *******************************************************************************************************\n",
    "    with codecs.open(NODES_PATH, 'w', \"utf-8\") as nodes_file, \\\n",
    "            codecs.open(NODE_TAGS_PATH, 'w', \"utf-8\") as nodes_tags_file, \\\n",
    "            codecs.open(WAYS_PATH, 'w', \"utf-8\") as ways_file, \\\n",
    "            codecs.open(WAY_NODES_PATH, 'w', \"utf-8\") as way_nodes_file, \\\n",
    "            codecs.open(WAY_TAGS_PATH, 'w', \"utf-8\") as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    fix.add_reno()\n",
    "    process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-corrections",
   "metadata": {},
   "source": [
    "## Created the .db file and read .csv to .db using python.\n",
    "\n",
    "I tried using one of my more favorite platforms, TablePlus, to create the database file but ran into quite a few problems specific to that program and decided to resort to python. This was a bit of a challenge with several bytes and string errors along the way but once I got one table to work right, the rest were easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "weekly-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"reno.db\")\n",
    "con.text_factory = str\n",
    "cur = con.cursor()\n",
    "\n",
    "# CREATE TABLE NODES\n",
    "cur.execute(\"create table nodes (id, lat, lon, user, uid, version, changeset, timestamp);\")\n",
    "with open('nodes.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) \\\n",
    "             for i in dr]\n",
    "\n",
    "cur.executemany(\"insert into nodes (id, lat, lon, user, uid, version, changeset, timestamp) \\\n",
    "                values (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "# CREATE TABLE NODES_TAGS\n",
    "cur.execute(\"create table nodes_tags (id, key, value, type);\")\n",
    "with open('nodes_tags.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"insert into nodes_tags (id, key, value, type) values (?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "# CREATE TABLE WAYS\n",
    "cur.execute(\"create table ways (id, user, uid, version, changeset, timestamp);\")\n",
    "with open('ways.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cur.executemany(\"insert into ways (id, user, uid, version, changeset, timestamp) values (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "# CREATE TABLE WAYS_NODES\n",
    "cur.execute(\"create table ways_nodes (id, node_id, position);\")\n",
    "with open('ways_nodes.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "\n",
    "cur.executemany(\"insert into ways_nodes (id, node_id, position) values (?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "# CREATE TABLE WAYS_TAGS\n",
    "cur.execute(\"create table ways_tags (id, key, value, type);\")\n",
    "with open('ways_tags.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"insert into ways_tags (id, key, value, type) values (?, ?, ?, ?);\", to_db)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-davis",
   "metadata": {},
   "source": [
    "# Overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "finite-director",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files for database...\n",
      "reno.osm : 222 MB\n",
      "reno.db : 153 MB\n",
      "ways.csv : 4 MB\n",
      "ways_tags.csv : 9 MB\n",
      "ways_nodes.csv : 29 MB\n",
      "nodes.csv : 89 MB\n",
      "nodes_tags.csv : 1 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "osm_path = 'reno.osm'\n",
    "db_path = 'reno.db'\n",
    "ways = 'ways.csv'\n",
    "ways_tags = 'ways_tags.csv'\n",
    "ways_nodes = 'ways_nodes.csv'\n",
    "nodes = 'nodes.csv'\n",
    "nodes_tags = 'nodes_tags.csv'\n",
    "print('Files for database...')\n",
    "print('reno.osm :', int(round(os.stat(osm_path).st_size) / (1024.0 * 1024.0)), 'MB')\n",
    "print('reno.db :', int(round(os.stat(db_path).st_size) / (1024.0 * 1024.0)), 'MB')\n",
    "print('ways.csv :', int(round(os.stat(ways).st_size) / (1024.0 * 1024.0)), 'MB')\n",
    "print('ways_tags.csv :', int(round(os.stat(ways_tags).st_size) / (1024.0 * 1024.0)), 'MB')\n",
    "print('ways_nodes.csv :', int(round(os.stat(ways_nodes).st_size) / (1024.0 * 1024.0)), 'MB')\n",
    "print('nodes.csv :', int(round(os.stat(nodes).st_size) / (1024.0 * 1024.0)), 'MB')\n",
    "print('nodes_tags.csv :', int(round(os.stat(nodes_tags).st_size) / (1024.0 * 1024.0)), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-product",
   "metadata": {},
   "source": [
    "### Loading sql and sql database than performing querys.\n",
    "\n",
    "In this section I load sqlite3 and conduct several queries for various info. I broke my queries out individually for a better viewing experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "presidential-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP FOR .DB CONNECTION AND QUERYS\n",
    "\n",
    "import sys\n",
    "\n",
    "import sqlite3\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "database_file = 'reno.db'\n",
    "conn = sqlite3.connect(database_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "crazy-granny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Total number of users contributing...'\n",
      "   count(sub.uid)\n",
      "0             828\n"
     ]
    }
   ],
   "source": [
    "def user_count():\n",
    "    uc = pd.read_sql_query(\"select count(sub.uid) \\\n",
    "                            from (select uid \\\n",
    "                            from nodes union \\\n",
    "                            select uid \\\n",
    "                            from ways) sub\", conn)\n",
    "    pprint(\"Total number of users contributing...\")\n",
    "    pprint(uc)\n",
    "user_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "express-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Total number of nodes...'\n",
      "   count(*)\n",
      "0   1107582\n"
     ]
    }
   ],
   "source": [
    "def nodes_count():\n",
    "    nc = pd.read_sql_query(\"select count(*) from nodes\", conn)\n",
    "    pprint(\"Total number of nodes...\")\n",
    "    pprint(nc)\n",
    "nodes_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sudden-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Total number of ways...'\n",
      "   count(*)\n",
      "0     77659\n"
     ]
    }
   ],
   "source": [
    "def ways_count():\n",
    "    wc = pd.read_sql_query(\"select count(*) from ways\", conn)\n",
    "    pprint(\"Total number of ways...\")\n",
    "    pprint(wc)\n",
    "ways_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "level-meter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Total number of amenities within sample.osm...'\n",
      "               value  num\n",
      "0         restaurant  121\n",
      "1              bench  110\n",
      "2             school   71\n",
      "3          fast_food   61\n",
      "4               fuel   38\n",
      "5                bar   31\n",
      "6   parking_entrance   30\n",
      "7               cafe   30\n",
      "8   charging_station   22\n",
      "9            toilets   19\n",
      "10              bank   19\n",
      "11          post_box   18\n",
      "12           shelter   16\n",
      "13               pub   12\n",
      "14          pharmacy   12\n"
     ]
    }
   ],
   "source": [
    "def amenities_count():\n",
    "    ac = pd.read_sql_query(\"select value, count(*) as num \\\n",
    "                            from nodes_tags where key = 'amenity' \\\n",
    "                            group by value \\\n",
    "                            order by num desc \\\n",
    "                            limit 15;\", conn)\n",
    "    pprint(\"Total number of amenities within sample.osm...\")\n",
    "    pprint(ac)\n",
    "amenities_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "scenic-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Top 15 postal codes and the counts within...'\n",
      "    value  count\n",
      "0   89509   2031\n",
      "1   89503    708\n",
      "2   89519    598\n",
      "3   89501    382\n",
      "4   89512    290\n",
      "5   89506    245\n",
      "6   89502    150\n",
      "7   89436    106\n",
      "8   89521     78\n",
      "9   89523     54\n",
      "10  89431     44\n",
      "11  89511     35\n",
      "12  89434     29\n",
      "13  89439     17\n",
      "14  96161     14\n"
     ]
    }
   ],
   "source": [
    "def postal_code_counts():\n",
    "    pcc = pd.read_sql_query(\"select subq.value, count(*) as count \\\n",
    "                             from (select * from nodes_tags union \\\n",
    "                             select * from ways_tags) \\\n",
    "                             subq where subq.key = 'postcode' \\\n",
    "                             group by subq.value \\\n",
    "                             order by count desc \\\n",
    "                             limit 15\", conn)\n",
    "    pprint(\"Top 15 postal codes and the counts within...\")\n",
    "    pprint(pcc)\n",
    "postal_code_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-functionality",
   "metadata": {},
   "source": [
    "## Other ideas about the datasets\n",
    "\n",
    "This represents a few other queries to see some other data viewpoints. These viewpoints represent more opportunities to clean our data. For example; in amenities count, we could potentially combine pub and bar but this may misrepresent the data as they could represent different entities, after all, in Reno, Nevada alcohol and liquor are literally sold everywhere and many casinos also have bars. Another example of opportunity rest in building type; there are not 132 physical universities in Reno, as a city in a valley, it just isn't big enough to have that many independent universities but we do have the University of Northern Reno (UNR) which could easily have 132 structures that could fall under the university umbrella. Many opportunities exist with this data set depending on the motives of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "pressed-recycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Places within sample taken from reno.osm...'\n",
      "               value                     value\n",
      "0           locality                    Hinton\n",
      "1      neighbourhood        Sierra Subdivision\n",
      "2  isolated_dwelling                   Iceland\n",
      "3             hamlet                   Peavine\n",
      "4      neighbourhood  Prosser Lakeview Estates\n",
      "5           locality              Hobart Mills\n",
      "6             hamlet                Hirschdale\n",
      "7           locality                    Mystic\n",
      "8           locality                      Boca\n",
      "9             hamlet                    Plumas\n"
     ]
    }
   ],
   "source": [
    "def get_places():\n",
    "    gp = pd.read_sql_query(\"select sub.value, nt.value \\\n",
    "                            from nodes_tags nt join \\\n",
    "                                (select id, value \\\n",
    "                                from nodes_tags \\\n",
    "                                where key = 'place') \\\n",
    "                            sub on nt.id = sub.id \\\n",
    "                            where nt.key = 'name' \\\n",
    "                            limit 10\", conn)\n",
    "    pprint(\"Places within sample taken from reno.osm...\")\n",
    "    pprint(gp)\n",
    "get_places()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "damaged-quantity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Top 5 buildings by type...'\n",
      "         value   num\n",
      "0        house  5327\n",
      "1   apartments   535\n",
      "2   commercial   382\n",
      "3       retail   271\n",
      "4         roof   243\n",
      "5       garage   178\n",
      "6   industrial   153\n",
      "7   university   132\n",
      "8         shed    91\n",
      "9       school    80\n",
      "10     terrace    62\n",
      "11   warehouse    46\n",
      "12     garages    37\n",
      "13      office    28\n",
      "14      hangar    27\n"
     ]
    }
   ],
   "source": [
    "def building_type():\n",
    "    bt = pd.read_sql_query(\"select tags.value, count(*) as num \\\n",
    "                            from (select * from nodes_tags union all select * from ways_tags) tags \\\n",
    "                            where tags.key = 'building' and tags.value != 'yes' \\\n",
    "                            group by tags.value \\\n",
    "                            order by num desc \\\n",
    "                            limit 15;\", conn)\n",
    "    pprint(\"Top 5 buildings by type...\")\n",
    "    pprint(bt)\n",
    "building_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "decimal-faculty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Top 5 sports land types...'\n",
      "             value  num\n",
      "0           tennis   98\n",
      "1         baseball   89\n",
      "2             golf   78\n",
      "3       basketball   64\n",
      "4       horseshoes   34\n",
      "5          karting   21\n",
      "6  beachvolleyball   15\n",
      "7           soccer   14\n",
      "8         swimming   10\n",
      "9       skateboard    9\n"
     ]
    }
   ],
   "source": [
    "def sports_types():\n",
    "    st = pd.read_sql_query(\"select tags.value, count(*) as num \\\n",
    "                            from (select * from nodes_tags union all \\\n",
    "                                select * from ways_tags) tags \\\n",
    "                            where tags.key = 'sport' and tags.value != 'yes' \\\n",
    "                            group by tags.value \\\n",
    "                            order by num desc \\\n",
    "                            limit 10;\", conn)\n",
    "    pprint(\"Top 5 sports land types...\")\n",
    "    pprint(st)\n",
    "sports_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fallen-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Some website's...\"\n",
      "                                               value\n",
      "0                           facebook.com/redrockreno\n",
      "1                                     greatbasin.org\n",
      "2                                http://adsd.nv.gov/\n",
      "3                          http://coneyislandbar.net\n",
      "4  http://corporate.ppg.com/Our-Company/Worldwide...\n",
      "5             http://deserthighlandsministorage.com/\n",
      "6                 http://discountwindowcoverings.org\n",
      "7  http://dpbh.nv.gov/About/Overview/NNAMHS_Overv...\n",
      "8                            http://drmccaskill.com/\n",
      "9                          http://golfwildcreek.com/\n"
     ]
    }
   ],
   "source": [
    "def web_site():\n",
    "    ws = pd.read_sql_query(\"select distinct tags.value \\\n",
    "                            from (select * from nodes_tags \\\n",
    "                                union all \\\n",
    "                                select * from ways_tags) tags \\\n",
    "                            where tags.key = 'website' \\\n",
    "                            order by tags.value \\\n",
    "                            limit 10;\", conn)\n",
    "    pprint(\"Some website's...\")\n",
    "    pprint(ws)\n",
    "web_site()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "meaningful-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Top 10 contributors...'\n",
      "              user     num\n",
      "0        btwhite92  507179\n",
      "1  woodpeck_fixbot  108440\n",
      "2         abschiff   63731\n",
      "3          nmixter   61840\n",
      "4         balcoath   57741\n",
      "5   BryanOSullivan   29792\n",
      "6         Phil O'D   25304\n",
      "7            mgwst   14956\n",
      "8          JPSReno   11367\n",
      "9     MatthewKluft   11359\n"
     ]
    }
   ],
   "source": [
    "def top_contrib():\n",
    "    tc = pd.read_sql_query(\"select e.user, count(*) as num \\\n",
    "                            from (select user from nodes union all \\\n",
    "                                  select user from ways) e \\\n",
    "                            group by e.user \\\n",
    "                            order by num desc \\\n",
    "                            limit 10;\", conn)\n",
    "    pprint(\"Top 10 contributors...\")\n",
    "    pprint(tc)\n",
    "top_contrib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "blessed-habitat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Eateries within sample.osm...'\n",
      "         value  num\n",
      "0        pizza   16\n",
      "1      chinese   10\n",
      "2      mexican    9\n",
      "3     japanese    6\n",
      "4      italian    5\n",
      "5       indian    4\n",
      "6     american    4\n",
      "7   vietnamese    3\n",
      "8     sandwich    3\n",
      "9  coffee_shop    3\n"
     ]
    }
   ],
   "source": [
    "def popular_eats():\n",
    "    pe = pd.read_sql_query(\"select nodes_tags.value, count(*) as num \\\n",
    "                            from nodes_tags \\\n",
    "                                join (select distinct(id) \\\n",
    "                                from nodes_tags \\\n",
    "                                where value='restaurant') i \\\n",
    "                                on nodes_tags.id=i.id \\\n",
    "                            where nodes_tags.key='cuisine' \\\n",
    "                            group by nodes_tags.value \\\n",
    "                            order by num desc \\\n",
    "                            limit 10;\", conn)\n",
    "    pprint(\"Eateries within sample.osm...\")\n",
    "    pprint(pe)\n",
    "popular_eats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-investing",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The sheer amount of data that makes up the various files was very eye-opening. I completely understand how a vast majority of a data analyst's time can be given to just trying to get the data clean. With that said, I believe this data is clean enough for the purpose of the class but when considering things like the websites, Wikipedia pages, and phone numbers; there is ample opportunity for much more wrangling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
